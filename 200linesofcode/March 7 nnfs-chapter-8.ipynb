{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-07T20:17:22.538260Z","iopub.execute_input":"2024-03-07T20:17:22.538635Z","iopub.status.idle":"2024-03-07T20:17:23.016877Z","shell.execute_reply.started":"2024-03-07T20:17:22.538607Z","shell.execute_reply":"2024-03-07T20:17:23.015681Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%pip install nnfs\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nnnfs.init()\n\n# Dense layer \nclass Layer_Dense:\n    \n    # layer initialization \n    def __init__(self, n_inputs, n_neurons):\n        \n        #initialize weights and biases\n        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n        \n    # forward pass\n    def forward(self, inputs):\n        \n        # calculate output values from inputs, weights, and biases\n        self.output = np.dot(inputs, self.weights) + self.biases\n        \n# ReLU activation\nclass Activation_ReLU:\n    \n    # forward pass\n    def forward(self, inputs):\n        \n        # calculate output values from inputs\n        self.output = np.maximum(0, inputs)\n        \n# Softmax activation\nclass Activation_Softmax:\n    \n    # forward pass\n    def forward(self, inputs):\n        \n        # get unnormalized probabilities \n        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n        \n        # normalize them for each value\n        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n        \n        self.output = probabilities\n\n# a common loss class\nclass Loss:\n    \n    def calculate(self, output, y):\n        \n        # calculate sample losses\n        sample_losses = self.forward(output, y)\n        \n        # calculate mean loss\n        data_loss = np.mean(sample_losses)\n        \n        # return loss\n        return data_loss\n    \n\nclass Loss_CategoricalCrossentropy(Loss):\n    \n    # forward pass\n    def forward(self, y_pred, y_true):\n        \n        # number of samples\n        samples = len(y_pred)\n        \n        # clip data to prevent a division by 0\n        # clip both sides to not drag mean towards any value\n        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n        \n        # probabilities for target values - \n        # only if categorical labels\n        if len(y_true.shape) == 1:\n            correct_confidences = y_pred_clipped[\n                range(samples),\n                y_true\n            ]\n            \n        # mask values - only for one-hot encoded labels\n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(\n                y_pred_clipped * y_true,\n                axis = 1\n            )\n            \n        # losses\n        negative_log_likelihoods = -np.log(correct_confidences)\n        return negative_log_likelihoods\n    \n    # backward pass\n    def backward(self, dvalues, y_true):\n        \n        # number of samples\n        samples = len(values)\n        \n        labels = len(dvalues[0])\n        \n        # if labels are sparse, turn them into one-hot vector\n        if len(y_true.shape) == 1:\n            y_true = np.eye(labels)[y_true]\n            \n        # calculate gradient\n        self.dinputs = -y_true / dvalues\n        \n        # normalize gradient\n        self.dinputs = self.dinputs / samples","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:57.413273Z","iopub.execute_input":"2024-03-07T21:09:57.413714Z","iopub.status.idle":"2024-03-07T21:10:10.007301Z","shell.execute_reply.started":"2024-03-07T21:09:57.413677Z","shell.execute_reply":"2024-03-07T21:10:10.004994Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nnfs in /opt/conda/lib/python3.10/site-packages (0.5.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nnfs) (1.26.4)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# go back to page 120 for the calculation\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2],\n[0.1, 0.5, 0.4],\n[0.02, 0.9, 0.08]])\nclass_targets = np.array([[1, 0, 0],\n[0, 1, 0],\n[0, 1, 0]])\n\n\nloss_function = Loss_CategoricalCrossentropy()\nloss = loss_function.calculate(softmax_outputs, class_targets)\nprint(loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:11:23.613208Z","iopub.execute_input":"2024-03-07T21:11:23.613687Z","iopub.status.idle":"2024-03-07T21:11:23.622672Z","shell.execute_reply.started":"2024-03-07T21:11:23.613647Z","shell.execute_reply":"2024-03-07T21:11:23.621375Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"0.38506088005216804\n","output_type":"stream"}]},{"cell_type":"code","source":"# try using the above functions\nX, y = spiral_data(samples = 100, classes = 3)\n\n# create Dense layer with 2 input features and 3 ouptut values\ndense1 = Layer_Dense(2, 3)\n\n# create ReLU activation (to be used with Dense layer)\nactivation1 = Activation_ReLU()\n\n# create second Dense layer with 3 input features (as we take output # of previous layer here) and 3 output values\ndense2 = Layer_Dense(3, 3)\n\n# create Softmax activation (to be used with Dense layer)\nactivation2 = Activation_Softmax()\n\n# create loss function \nloss_function = Loss_CategoricalCrossentropy()\n\n# perform a forward pass of our training data through this layer\ndense1.forward(X)\n\n# perform a forward pass through activation function\n# it takes the output of first dense layer here\nactivation1.forward(dense1.output)\n\n# perform a forward pass through the second Dense layer\n# it takes outputs of activation function of first layer as inputs \ndense2.forward(activation1.output)\n\n# perform a forward pass through activation function\n# it takes the output of second Dense layer here\nactivation2.forward(dense2.output)\n\n# Let's see output of the first few examples\nprint(activation2.output[:5])","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:11:25.993539Z","iopub.execute_input":"2024-03-07T21:11:25.994486Z","iopub.status.idle":"2024-03-07T21:11:26.009566Z","shell.execute_reply.started":"2024-03-07T21:11:25.994441Z","shell.execute_reply":"2024-03-07T21:11:26.008310Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[[0.33333334 0.33333334 0.33333334]\n [0.3333332  0.3333332  0.33333364]\n [0.3333329  0.33333293 0.3333342 ]\n [0.3333326  0.33333263 0.33333477]\n [0.33333233 0.3333324  0.33333528]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# perform a forward pass through the loss function\n# it takes the output of second dense layer here and returns loss\nloss = loss_function.calculate(activation2.output, y)\n\n# print loss value\nprint('loss: ', loss)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:12:32.324426Z","iopub.execute_input":"2024-03-07T21:12:32.324844Z","iopub.status.idle":"2024-03-07T21:12:32.332034Z","shell.execute_reply.started":"2024-03-07T21:12:32.324809Z","shell.execute_reply":"2024-03-07T21:12:32.330419Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"loss:  1.0986104\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}