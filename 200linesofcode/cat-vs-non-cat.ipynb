{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7529886,"sourceType":"datasetVersion","datasetId":4385731}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nimport time\nfrom PIL import Image\nfrom scipy import ndimage\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\nnp.random.seed(3)\n\ndef relu(x):\n    return np.maximum(0, x)\n    \ndef sigmoid(x):\n    return 1/(1+ np.exp(-x))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-01T23:29:10.112868Z","iopub.execute_input":"2024-02-01T23:29:10.113617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_parameters(n_x, n_h, n_y):\n    W1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01\n    b2 = np.zeros((n_y, 1))\n    \n    assert(W1.shape == (n_h, n_x))\n    assert(b1.shape == (n_h, 1))\n    assert(W2.shape == (n_y, n_h))\n    assert(b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters \n\ndef initialize_parameters_deep(layer_dims):\n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)\n    \n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n        \n    return parameters\n\ndef linear_forward(A, W, b):\n    Z = np.dot(W, A) + b\n    cache = (A, W, b)\n    return Z, cache\n\ndef linear_activation_forward(A_prev, W, b, activation):\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A = sigmoid(Z)\n        activation_cache = Z\n        \n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A = relu(Z)\n        activation_cache = Z\n        \n    cache = (linear_cache, activation_cache)\n    \n    return A, cache\n\ndef L_model_forward(X, parameters):\n    # implement forward propagation for the [LINEAR -> RELU]*(L-1)->LINEAR->SIGMOID computation\n    caches = []\n    A = X\n    L = len(parameters) // 2\n    \n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n        caches.append(cache)\n    \n    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n    caches.append(cache)\n    \n    return AL, caches\n\ndef compute_cost(AL, Y):\n    m = Y.shape[1]\n    cost = -(1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n    \n    cost = np.squeeze(cost)\n    \n    return cost\n\ndef linear_backward(dZ, cache):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    \n    dW = (1/m)*np.dot(dZ, A_prev.T)\n    db = (1/m)*np.sum(dZ, axis = 1, keepdims = True)\n    dA_prev = np.dot(W.T, dZ)\n    \n    return dA_prev, dW, db\n\ndef linear_activation_backward(dA, cache, activation):\n    \n    linear_cache, activation_cache = cache \n    \n    if activation == \"relu\":\n        dZ = dA*np.where(activation_cache > 0, 1, 0)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    \n    if activation == \"sigmoid\":\n        dZ = dA*sigmoid(activation_cache)*(1 - sigmoid(activation_cache))\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    return dA_prev, dW, db\n\ndef L_model_backward(AL, Y, caches):\n    grads = {}\n    L = len(caches)\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape)\n    \n    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n    current_cache = caches[-1]\n    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n    grads[\"dA\" + str(L-1)] = dA_prev_temp\n    grads[\"dW\" + str(L)] = dW_temp\n    grads[\"db\" + str(L)] = db_temp\n    \n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], caches[l], 'relu')\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n        \n    return grads\n\ndef update_parameters(params, grads, learning_rate):\n    parameters = copy.deepcopy(params)\n    L = len(parameters)\n    \n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate*grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate*grads[\"db\" + str(l + 1)]\n    \n    return parameters\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = h5py.File(\"/kaggle/input/cat-v-non-cat/train_catvnoncat.h5\")\ntest = h5py.File(\"/kaggle/input/cat-v-non-cat/test_catvnoncat.h5\")\n\ntrain_x_orig = np.array(train[\"train_set_x\"][:])\ntrain_y_orig = np.array(train[\"train_set_y\"][:])\n\ntest_x_orig = np.array(test[\"test_set_x\"][:])\ntest_y_orig = np.array(test[\"test_set_y\"][:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(train_x_orig[10])\nprint(\"train_x has shape: \" + str(train_x_orig.shape))\nprint(\"test_x has shape: \" + str(test_x_orig.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use a L-layer neural network to choose between cat vs. non-cat images\n\n# first, reshape and standardize the images before feeding them to the network \ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\ntrain_x = train_x_flatten/255\ntest_x = test_x_flatten/255\n\ntrain_y = train_y_orig.reshape(1, 209)\ntest_y = test_y_orig.reshape(1, 50)\n\nprint (\"train_x's shape: \" + str(train_x.shape))\nprint (\"test_x's shape: \" + str(test_x.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting up the L-layer network, why are the layers dimensions set as below?\nimport copy\nlayers_dims = [12288, 20, 7, 5, 1]\n\ndef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost = False):\n    np.random.seed(3)\n    costs = []\n    \n    parameters = initialize_parameters_deep(layers_dims)\n    \n    for i in range(0, num_iterations):\n        AL, caches = L_model_forward(X, parameters)\n        \n        cost = compute_cost(AL, Y)\n        \n        grads = L_model_backward(AL, Y, caches)\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        if print_cost and i % 100 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n            \n    return parameters, costs\n\nparameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}